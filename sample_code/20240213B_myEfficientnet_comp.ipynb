{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c4b3af0-8796-4f91-a3b8-a189ed4b5b47",
   "metadata": {},
   "source": [
    "# 自前のネットワークを組んで学習させてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7703dc5b-ee9d-4aaa-add2-f96d5029ac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "plt.rcParams['font.family'] = 'Meiryo'\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41647b1b-afa7-4fd6-ae50-20a1694f1918",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn = r'./dataset_cur_train.csv'\n",
    "test_fn = r'./dataset_cur_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55fd0446-9e09-443b-9a85-463eec4bdbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dir_path_recursive = './myEfficientnet/'\n",
    "os.makedirs(new_dir_path_recursive, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5294d05-3e0d-461d-83fc-16ebda6d1449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "red_diff             float32\n",
       "remain_ends          float32\n",
       "last_stone_is_red    float32\n",
       "red_postion          float32\n",
       "filepath              object\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>red_diff</th>\n",
       "      <th>remain_ends</th>\n",
       "      <th>last_stone_is_red</th>\n",
       "      <th>red_postion</th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>./dataset_o\\ECC2023_ResultsBook_Women_A-Divisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>./dataset_o\\ECC2021_ResultsBook_Men_A-Division...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>./dataset_o\\ECC2019_ResultsBook_Women_A-Divisi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   red_diff  remain_ends  last_stone_is_red  red_postion  \\\n",
       "8       1.0          0.0                1.0          3.0   \n",
       "0       2.0          8.0                1.0          0.0   \n",
       "8      -2.0          0.0                0.0          3.0   \n",
       "\n",
       "                                            filepath  \n",
       "8  ./dataset_o\\ECC2023_ResultsBook_Women_A-Divisi...  \n",
       "0  ./dataset_o\\ECC2021_ResultsBook_Men_A-Division...  \n",
       "8  ./dataset_o\\ECC2019_ResultsBook_Women_A-Divisi...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = pd.read_csv(train_fn,index_col=0)\n",
    "df_train['red_diff'] = df_train['red_diff'].astype(np.float32)\n",
    "df_train['remain_ends'] = df_train['remain_ends'].astype(np.float32)\n",
    "df_train['last_stone_is_red'] = df_train['last_stone_is_red'].astype(np.float32)\n",
    "df_train['red_postion'] = df_train['red_postion'].astype(np.float32)\n",
    "display(df_train.dtypes)\n",
    "display(df_train.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2416d239-816d-4246-98c9-68aa35baafb0",
   "metadata": {},
   "source": [
    "## red_diff が目的、fn,remain_ends,last_stone_is_red,red_postion が説明変数\n",
    "説明変数を標準化する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba7ce319-2e82-424f-94ff-2a6171635c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5558919 ,  0.9972611 ,  1.2323233 ],\n",
       "       [ 1.5270197 ,  0.9972611 , -0.01566701],\n",
       "       [-1.5558919 , -1.0027465 ,  1.2323233 ]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['remain_ends', 'last_stone_is_red', 'red_postion']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([4.0374607 , 0.50137133, 0.03766138])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([6.73376293, 0.24999812, 5.7785669 ])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>red_diff</th>\n",
       "      <th>remain_ends</th>\n",
       "      <th>last_stone_is_red</th>\n",
       "      <th>red_postion</th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.555892</td>\n",
       "      <td>0.997261</td>\n",
       "      <td>1.232323</td>\n",
       "      <td>./dataset_o\\ECC2023_ResultsBook_Women_A-Divisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.527020</td>\n",
       "      <td>0.997261</td>\n",
       "      <td>-0.015667</td>\n",
       "      <td>./dataset_o\\ECC2021_ResultsBook_Men_A-Division...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.555892</td>\n",
       "      <td>-1.002746</td>\n",
       "      <td>1.232323</td>\n",
       "      <td>./dataset_o\\ECC2019_ResultsBook_Women_A-Divisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.170528</td>\n",
       "      <td>0.997261</td>\n",
       "      <td>-0.431664</td>\n",
       "      <td>./dataset_o\\OWG2018_ResultsBook\\geme835end8.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-3.0</td>\n",
       "      <td>-1.555892</td>\n",
       "      <td>-1.002746</td>\n",
       "      <td>0.400330</td>\n",
       "      <td>./dataset_o\\WWCC2018_ResultsBook\\geme338end9.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   red_diff  remain_ends  last_stone_is_red  red_postion  \\\n",
       "8       1.0    -1.555892           0.997261     1.232323   \n",
       "0       2.0     1.527020           0.997261    -0.015667   \n",
       "8      -2.0    -1.555892          -1.002746     1.232323   \n",
       "7       1.0    -1.170528           0.997261    -0.431664   \n",
       "8      -3.0    -1.555892          -1.002746     0.400330   \n",
       "\n",
       "                                            filepath  \n",
       "8  ./dataset_o\\ECC2023_ResultsBook_Women_A-Divisi...  \n",
       "0  ./dataset_o\\ECC2021_ResultsBook_Men_A-Division...  \n",
       "8  ./dataset_o\\ECC2019_ResultsBook_Women_A-Divisi...  \n",
       "7    ./dataset_o\\OWG2018_ResultsBook\\geme835end8.png  \n",
       "8   ./dataset_o\\WWCC2018_ResultsBook\\geme338end9.png  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 標準化\n",
    "stdsc = StandardScaler()\n",
    "##学習時の標準化したパラメータは、評価、本番時におなじ重みで標準化する処理が必要\n",
    "x_train_df = df_train.copy().drop(['filepath','red_diff'],axis=1)\n",
    "x_train_std = stdsc.fit_transform(x_train_df)\n",
    "display( x_train_std[:3] )\n",
    "## DataFrameの値を入れ替え\n",
    "qcl = df_train.columns.to_list()\n",
    "qcl.remove('filepath')\n",
    "qcl.remove('red_diff')\n",
    "print(qcl)\n",
    "df_train[qcl] = x_train_std\n",
    "pickle.dump(stdsc, open(new_dir_path_recursive+\"stdsc_02240209.pkl\", \"wb\"))\n",
    "display(stdsc.n_features_in_, stdsc.mean_ , stdsc.var_) \n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232f4a74-d1e7-4304-a0f7-933485b4a9ba",
   "metadata": {},
   "source": [
    "### RESNET18\n",
    "https://pytorch.org/vision/stable/models/generated/torchvision.models.efficientnet_v2_s.html#torchvision.models.EfficientNet_V2_S_Weights  \n",
    "crop = 384\n",
    "mean=[0.485, 0.456, 0.406] , std=[0.229, 0.224, 0.225]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc251581-3052-406f-94bf-c781ea7943a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_fix_seed(seed=42):\n",
    "    # Python random\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Pytorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    ##torch.backends.cudnn.deterministic = True\n",
    "    ##torch.use_deterministic_algorithms = True\n",
    "\n",
    "torch_fix_seed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d98501e-1d3d-4b23-bb83-646cf5a3f981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[300, 540]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "120.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 画像変換の定義\n",
    "w,h = Image.open(df_train['filepath'].values[0]).size\n",
    "## 正方形にするための差分\n",
    "pad = (h-w)/2\n",
    "display([w,h],pad)\n",
    "target_size = 384\n",
    "## train用\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Pad(( 240 // 2, 0), fill=0, padding_mode='constant'),  # 左右に余白を追加\n",
    "    transforms.Resize(target_size),\n",
    "    transforms.CenterCrop(target_size),\n",
    "    transforms.RandomHorizontalFlip(0.33),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "# valid/test用\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Pad(( 240 // 2, 0), fill=0, padding_mode='constant'),  # 左右に余白を追加\n",
    "    transforms.Resize(target_size),\n",
    "    transforms.CenterCrop(target_size),\n",
    "    transforms.RandomHorizontalFlip(0.33),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f5154d-84a6-42eb-b638-dbab73d94516",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9f773c-6504-40e9-a103-bd8b100ab1a2",
   "metadata": {},
   "source": [
    "# 自前のデータセット定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f04521e7-c7bd-4a5d-9a0c-71a00502dff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgValueDataset(Dataset):\n",
    "    def __init__(self, df, classcol , fncol , transform):\n",
    "        \n",
    "        ##self.label_list  = df[classcol].to_list()\n",
    "        class_data = pd.get_dummies(df[classcol]).values\n",
    "        self.label_list  = class_data.astype(float)\n",
    "\n",
    "        self.val_list  = df[['remain_ends','last_stone_is_red','red_postion']].astype(np.float16).values\n",
    "        \n",
    "        self.img_pathlist  = df[fncol].to_list()\n",
    "        cols = df.columns.to_list()\n",
    "        cols.remove(fncol)\n",
    "        cols.remove(classcol)\n",
    "        self.x_values = df[cols].values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):  \n",
    "        return len( self.img_pathlist )\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # 画像をPILとして読み込む\n",
    "        #print(index,self.img_pathlist[index])\n",
    "        image = Image.open(self.img_pathlist[index])\n",
    "        \n",
    "        label = self.label_list[index]\n",
    "\n",
    "        extend = self.val_list[index]                         \n",
    "        if self.transform is not None:\n",
    "            ##print('use transform')\n",
    "            image = self.transform(image)\n",
    "        ## 次元を足してやってっそこに追加データをぶっこむ\n",
    "        extend_tensor = np.full((target_size,target_size),255)\n",
    "        extend_tensor[1][0] = extend[0]\n",
    "        extend_tensor[1][1] = extend[1]\n",
    "        extend_tensor[1][2] = extend[2]\n",
    "        #print(extend_tensor)\n",
    "        extend_tensor = torch.Tensor(extend_tensor)\n",
    "        extend_tensor = extend_tensor.unsqueeze(0)\n",
    "        out = torch.cat([image, extend_tensor], dim=0)\n",
    "                             \n",
    "        return out, label \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1a795a1-a4b0-4f93-bb8d-9544cce15f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11959"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df_train, test_size=0.2, stratify=df_train['red_diff'])\n",
    "train_dataset = ImgValueDataset( train_df ,classcol='red_diff' , fncol='filepath',transform=transform_train)\n",
    "test_dataset = ImgValueDataset( test_df ,classcol='red_diff' , fncol='filepath' ,transform=transform_test)\n",
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a9aa0e0-f19c-4983-846a-4fb6955f5645",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f12baa4-eaf9-43e9-aed0-30be6dabc706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2990"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b80edabd-3b50-49e8-aec9-88cb63cfa1b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 4, 384, 384]),\n",
       " tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, labels = next(iter(train_loader))\n",
    "inputs.shape , labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29ca2083-5c34-403f-9d51-ec37ea1750f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 384, 384])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = inputs[:3]\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10aa115c-2356-43c7-a7b5-58c3fe577e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[255., 255., 255.,  ..., 255., 255., 255.],\n",
       "        [  0.,  -1.,   0.,  ..., 255., 255., 255.],\n",
       "        [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "        ...,\n",
       "        [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "        [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "        [255., 255., 255.,  ..., 255., 255., 255.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[2,3,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "239d9d73-c67f-469a-aca2-649351d9deb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 384, 384]) tensor([[ 0., -1.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "values = inputs[3:]\n",
    "print(values.shape , values[:,3,1,0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0d21d0a-ca15-4f11-81b1-6603377007d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.,  0.,  2.,  1., -2., -4.,  3., -3.,  4.,  5., -5.],\n",
       "       dtype=float32),\n",
       " 11)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_list = train_df['red_diff'].unique()\n",
    "class_list , len(class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d03684ed-0a69-4157-a7d4-ceb1bd546cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0., -1.,  0.],\n",
       "        [ 1.,  0.,  0.],\n",
       "        [ 0., -1.,  0.],\n",
       "        [ 0., -1.,  0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[:,3,1,0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14f0cda-b0ce-4e75-a54d-7cbc9ac14da9",
   "metadata": {},
   "source": [
    "# ネットを組む  \n",
    "https://qiita.com/poorko/items/c151ff4a827f114fe954"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1b9b5f2-4064-4e3d-861a-bc23310ac89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c24fd3c2-85ef-4dc3-a15a-02ce1b0d1624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./myEfficientnet\\efficientnet_v2_s-dd5fe13b.pth\n",
      "<class 'torchvision.models.efficientnet.EfficientNet'>\n",
      "tensor([-0.0237, -0.0518, -0.0481, -0.0520, -0.0489,  0.0257, -0.0434, -0.0217,\n",
      "        -0.0387, -0.0204], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model_ft = models.efficientnet_v2_s(pretrained=False)\n",
    "g = glob.glob(new_dir_path_recursive + '*.pth')\n",
    "fn = g[0]\n",
    "print(fn)\n",
    "model_ft.load_state_dict(torch.load(g[0]))\n",
    "print(type(model_ft))\n",
    "print(model_ft.classifier[1].bias[:10] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "07f4df3f-61b7-48e9-824f-3fe814bfa658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./myEfficientnet\\efficientnet_v2_s-dd5fe13b.pth\n",
      "<class 'torchvision.models.efficientnet.EfficientNet'> 1280\n",
      "tensor([-0.0237, -0.0518, -0.0481, -0.0520, -0.0489,  0.0257, -0.0434, -0.0217,\n",
      "        -0.0387, -0.0204], grad_fn=<SliceBackward0>)\n",
      "すべてのネットの重みをロック [ True]\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models.efficientnet import EfficientNet,MBConvConfig,FusedMBConvConfig\n",
    "from torchvision.models.efficientnet import _efficientnet_conf\n",
    "\n",
    "torchvision.models.efficientne.EfficientNet\n",
    "\n",
    "inverted_residual_setting, last_channel = _efficientnet_conf(\"efficientnet_v2_s\")\n",
    "net = EfficientNet(inverted_residual_setting = inverted_residual_setting , \n",
    "                   dropout = 0.2,\n",
    "                   last_channel = last_channel)\n",
    "print(g[0])\n",
    "net.load_state_dict(torch.load(g[0]))\n",
    "print(type(net),last_channel)\n",
    "print(net.classifier[1].bias[:10] )\n",
    "\n",
    "for i, param in enumerate(net.parameters()):\n",
    "        param.requires_grad = True #False\n",
    "x = [ param.requires_grad for param in net.parameters()]\n",
    "print('すべてのネットの重みをロック',np.unique(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1fdeb7e6-f063-42ea-8361-8f79f299957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _myEfficientNet(EfficientNet):\n",
    "    def __init__(self, inverted_residual_setting,dropout,last_channel):\n",
    "        super(_myEfficientNet, self).__init__(inverted_residual_setting,dropout,last_channel)\n",
    "        print(_myEfficientNet)\n",
    "        pass\n",
    "    def load_state_dict(self, state_dict,\n",
    "                        strict: bool = True, assign: bool = False):\n",
    "        super().load_state_dict( state_dict , strict , assign )\n",
    "        pass\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f60c28b0-cb80-43d3-9e4d-e950843e0024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280\n",
      "<class '__main__._myEfficientNet'>\n",
      "./myEfficientnet\\efficientnet_v2_s-dd5fe13b.pth\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for _myEfficientNet:\n\tsize mismatch for features.7.0.weight: copying a param with shape torch.Size([1280, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([1024, 256, 1, 1]).\n\tsize mismatch for features.7.1.weight: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for features.7.1.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for features.7.1.running_mean: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for features.7.1.running_var: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for classifier.1.weight: copying a param with shape torch.Size([1000, 1280]) from checkpoint, the shape in current model is torch.Size([1000, 1024]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(g[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#print(len(weights.meta[\"categories\"]))\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(net),last_channel)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(net\u001b[38;5;241m.\u001b[39mclassifier[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mbias[:\u001b[38;5;241m10\u001b[39m] )\n",
      "Cell \u001b[1;32mIn[64], line 8\u001b[0m, in \u001b[0;36m_myEfficientNet.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_state_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m, state_dict,\n\u001b[0;32m      7\u001b[0m                     strict: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, assign: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massign\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\dl_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2147\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2148\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2149\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2153\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for _myEfficientNet:\n\tsize mismatch for features.7.0.weight: copying a param with shape torch.Size([1280, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([1024, 256, 1, 1]).\n\tsize mismatch for features.7.1.weight: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for features.7.1.bias: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for features.7.1.running_mean: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for features.7.1.running_var: copying a param with shape torch.Size([1280]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for classifier.1.weight: copying a param with shape torch.Size([1000, 1280]) from checkpoint, the shape in current model is torch.Size([1000, 1024])."
     ]
    }
   ],
   "source": [
    "from torchvision.models.efficientnet import EfficientNet,MBConvConfig,FusedMBConvConfig\n",
    "from torchvision.models.efficientnet import _efficientnet_conf\n",
    "\n",
    "inverted_residual_setting, last_channel = _efficientnet_conf(\"efficientnet_v2_s\")\n",
    "print(last_channel)\n",
    "net = _myEfficientNet(inverted_residual_setting = inverted_residual_setting , \n",
    "                   dropout = 0.2,\n",
    "                   last_channel = last_channel)\n",
    "print(g[0])\n",
    "weights = torch.load(g[0])\n",
    "#print(len(weights.meta[\"categories\"]))\n",
    "net.load_state_dict( torch.load(g[0]) )\n",
    "print(type(net),last_channel)\n",
    "print(net.classifier[1].bias[:10] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "173ab9ac-2c15-448d-bb34-ea832749bd7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FusedMBConvConfig(expand_ratio=1, kernel=3, stride=1, input_channels=24, out_channels=24, num_layers=2, block=<class 'torchvision.models.efficientnet.FusedMBConv'>),\n",
       " FusedMBConvConfig(expand_ratio=4, kernel=3, stride=2, input_channels=24, out_channels=48, num_layers=4, block=<class 'torchvision.models.efficientnet.FusedMBConv'>),\n",
       " FusedMBConvConfig(expand_ratio=4, kernel=3, stride=2, input_channels=48, out_channels=64, num_layers=4, block=<class 'torchvision.models.efficientnet.FusedMBConv'>),\n",
       " MBConvConfig(expand_ratio=4, kernel=3, stride=2, input_channels=64, out_channels=128, num_layers=6, block=<class 'torchvision.models.efficientnet.MBConv'>),\n",
       " MBConvConfig(expand_ratio=6, kernel=3, stride=1, input_channels=128, out_channels=160, num_layers=9, block=<class 'torchvision.models.efficientnet.MBConv'>),\n",
       " MBConvConfig(expand_ratio=6, kernel=3, stride=2, input_channels=160, out_channels=256, num_layers=15, block=<class 'torchvision.models.efficientnet.MBConv'>)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_residual_setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27da4588-b7ef-42ff-a8c2-0951db5e44d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNNモデルの定義\n",
    "class myResnet18(models.resnet.ResNet):\n",
    "    def __init__(self, block,layers,num_classes):\n",
    "        super().__init__(block,layers,num_classes)\n",
    "        pass\n",
    "\n",
    "    def forward(self, xpacked:torch.Tensor) -> torch.Tensor:\n",
    "        input = xpacked\n",
    "        #print('input:',input.shape)\n",
    "        x = input[:,:3]\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        ###########################################\n",
    "        params = input[:,3,1,0:3]\n",
    "        output = torch.cat([x,params],1)\n",
    "        \n",
    "        ###########################################\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44185bb6-5b7a-41fc-bb5b-37e21e21a5b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.models.resnet.ResNet'> tensor([-0.0026,  0.0030,  0.0007, -0.0269,  0.0064,  0.0133, -0.0112,  0.0206,\n",
      "        -0.0036, -0.0123], grad_fn=<SliceBackward0>)\n",
      "すべてのネットの重みをロック [False]\n",
      "最後の一つを切り替えたので混在するのが正しい: [False  True]\n"
     ]
    }
   ],
   "source": [
    "net = myResnet18(block=BasicBlock,layers=[2, 2, 2, 2],num_classes=1000)\n",
    "net.load_state_dict(torch.load(g[0]))\n",
    "print(type(model_ft),model_ft.fc.bias[:10] )\n",
    "\n",
    "## 重みロック\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "x = [ param.requires_grad for param in net.parameters()]\n",
    "print('すべてのネットの重みをロック',np.unique(x))\n",
    "## 最終段を11にする\n",
    "num_ftrs = net.fc.in_features\n",
    "net.fc = nn.Linear(num_ftrs+3, 11)\n",
    "##display(net)\n",
    "x = [ param.requires_grad for param in net.parameters()]\n",
    "print('最後の一つを切り替えたので混在するのが正しい:',np.unique(x)) ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56df7713-f444-4187-afcb-ca5b5001f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 関数化して\n",
    "def createMyNet(num_class):\n",
    "    net = myResnet18(block=BasicBlock,layers=[2, 2, 2, 2],num_classes=1000)\n",
    "    net.load_state_dict(torch.load(g[0]))\n",
    "    \n",
    "    ## 重みロック\n",
    "    for param in net.parameters():\n",
    "        param.requires_grad = False\n",
    "    ## 最終段を11にする\n",
    "    num_ftrs = net.fc.in_features\n",
    "    net.fc = nn.Linear(num_ftrs+3, num_class)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec0cdb6-eb60-4e7f-affa-9543292a91f7",
   "metadata": {},
   "source": [
    "https://qiita.com/ku_a_i/items/ba33c9ce3449da23b503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "176fad73-44a8-4be0-adec-1a7b5b9f13e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"earlystoppingクラス\"\"\"\n",
    "\n",
    "    def __init__(self, patience=5, verbose=False, path='checkpoint_model.pth'):\n",
    "        \"\"\"引数：最小値の非更新数カウンタ、表示設定、モデル格納path\"\"\"\n",
    "\n",
    "        self.patience = patience    #設定ストップカウンタ\n",
    "        self.verbose = verbose      #表示の有無\n",
    "        self.counter = 0            #現在のカウンタ値\n",
    "        self.best_score = None      #ベストスコア\n",
    "        self.early_stop = False     #ストップフラグ\n",
    "        self.val_loss_min = np.Inf   #前回のベストスコア記憶用\n",
    "        self.path = path             #ベストモデル格納path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        特殊(call)メソッド\n",
    "        実際に学習ループ内で最小lossを更新したか否かを計算させる部分\n",
    "        \"\"\"\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:  #1Epoch目の処理\n",
    "            self.best_score = score   #1Epoch目はそのままベストスコアとして記録する\n",
    "            self.checkpoint(val_loss, model)  #記録後にモデルを保存してスコア表示する\n",
    "        elif score < self.best_score:  # ベストスコアを更新できなかった場合\n",
    "            self.counter += 1   #ストップカウンタを+1\n",
    "            if self.verbose:  #表示を有効にした場合は経過を表示\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')  #現在のカウンタを表示する \n",
    "            if self.counter >= self.patience:  #設定カウントを上回ったらストップフラグをTrueに変更\n",
    "                self.early_stop = True\n",
    "        else:  #ベストスコアを更新した場合\n",
    "            self.best_score = score  #ベストスコアを上書き\n",
    "            self.checkpoint(val_loss, model)  #モデルを保存してスコア表示\n",
    "            self.counter = 0  #ストップカウンタリセット\n",
    "\n",
    "    def checkpoint(self, val_loss, model):\n",
    "        '''ベストスコア更新時に実行されるチェックポイント関数'''\n",
    "        if self.verbose:  #表示を有効にした場合は、前回のベストスコアからどれだけ更新したか？を表示\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)  #ベストモデルを指定したpathに保存\n",
    "        self.val_loss_min = val_loss  #その時のlossを記録する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a382c74a-027d-41b9-a696-ae720728ab62",
   "metadata": {},
   "source": [
    "## 訓練（ネットワークが通るか確認）  \n",
    "https://atmarkit.itmedia.co.jp/ait/articles/2006/12/news021.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7972682-c9f2-40d5-ace5-b65335e99cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(values1, values2, rng, label1, label2):\n",
    "    plt.plot(range(rng), values1, label=label1)\n",
    "    plt.plot(range(rng), values2, label=label2)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "992b7ddd-0733-45d8-a7de-8dfaae7afde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, dataloader, criterion, optimizer,device):\n",
    "    net.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        #print('入力:',inputs.shape , labels.shape,labels.dtype)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        #print('評価:',outputs.shape , labels.shape,labels.dtype)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        _, ons_labels = torch.max(labels, 1)\n",
    "        total_correct += (predicted == ons_labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    accuracy = total_correct / len(dataloader.dataset)\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def validate(net, dataloader, criterion,device):\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, ons_labels = torch.max(labels, 1)\n",
    "            total_correct += (predicted == ons_labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    avg_accuracy = total_correct / len(dataloader.dataset)\n",
    "\n",
    "    return avg_loss, avg_accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1bda984b-5b9b-4047-947d-0fdeab431862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_train_and_validate(net, trainset, criterion, optimizer,scheduler, epochs,device):\n",
    "    best_accuracy = 0.0\n",
    "    trainloader, validloader = trainset\n",
    "    curEarlyStopping = EarlyStopping(patience=10, verbose=True,path=new_dir_path_recursive+'checkpoint_model.pth')\n",
    "\n",
    "\n",
    "    history = {}\n",
    "    history['train_loss_values'] = []\n",
    "    history['train_accuracy_values'] = []\n",
    "    history['valid_loss_values'] = []\n",
    "    history['valid_accuracy_values'] = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        current_time = datetime.now()\n",
    "        formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f'epoch: {epoch:3} <{formatted_time}>')\n",
    "\n",
    "        t_loss, t_accu = train(net, trainloader, criterion, optimizer,device)\n",
    "        v_loss, v_accu = validate(net, validloader, criterion,device)\n",
    "\n",
    "        print(f'train_loss: {t_loss:.6f}, train_accuracy: {t_accu:3.4%},',\n",
    "              f'valid_loss: {v_loss:.6f}, valid_accuracy: {v_accu:3.4%}')\n",
    "\n",
    "        history['train_loss_values'].append(t_loss)\n",
    "        history['train_accuracy_values'].append(t_accu)\n",
    "        history['valid_loss_values'].append(v_loss)\n",
    "        history['valid_accuracy_values'].append(v_accu)\n",
    "        pd.DataFrame(history).to_csv(new_dir_path_recursive+'mynet_spec.csv')\n",
    "        if best_accuracy < v_accu:\n",
    "            best_accuracy = v_accu\n",
    "            model_scripted = torch.jit.script(net)\n",
    "            model_scripted.save(new_dir_path_recursive+f'model_scripted_{epoch}.smodel.pth')\n",
    "        # 学習率の動的変更\n",
    "        scheduler.step(v_loss)\n",
    "        #★毎エポックearlystoppingの判定をさせる★\n",
    "        curEarlyStopping( v_loss , net) #callメソッド呼び出し\n",
    "        if curEarlyStopping.early_stop: #ストップフラグがTrueの場合、breakでforループを抜ける\n",
    "            print(\"Early Stopping!\")\n",
    "            return history\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ffae3a-8405-4241-af03-baac72b2b633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "epoch:   1 <2024-02-14 04:20:11>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset = ImgValueDataset( train_df ,classcol='red_diff' , fncol='filepath',transform=transform_train)\n",
    "test_dataset = ImgValueDataset( test_df ,classcol='red_diff' , fncol='filepath' ,transform=transform_test)\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0,drop_last = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "net = createMyNet(11).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True)\n",
    "\n",
    "EPOCHS = 300\n",
    "trainset = [train_loader,test_loader]\n",
    "\n",
    "history = do_train_and_validate(net, trainset, criterion, optimizer, scheduler,EPOCHS,device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b0dd1f-1662-4a29-b47b-615d3cf5223b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70005ff-3f5c-4062-a0ca-f193722dd117",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_losses = history['train_loss_values']\n",
    "t_accus = history['train_accuracy_values']\n",
    "v_losses = history['valid_loss_values']\n",
    "v_accus = history['valid_accuracy_values']\n",
    "\n",
    "plot_graph(t_losses, v_losses, len(t_losses), 'loss(train)', 'loss(validate)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0472e50-93a4-48f5-be84-3c8c38ed54c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(t_accus, v_accus, len(t_accus), 'accuracy(train)', 'accuracy(validate)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ef34ff-271c-40b8-86c8-584babfeca97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b7ff060-9a66-475e-b945-232d7afd9700",
   "metadata": {},
   "source": [
    "## 取り置きのテスト画像群"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3024cdab-7bc8-4c08-a737-90d25fcdfc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fn = r'./dataset_cur_test.csv'\n",
    "test_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b04518-7ea4-418d-8034-ce04bd42988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(test_fn,index_col=0)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "df_test['red_diff'] = df_test['red_diff'].astype(np.float32)\n",
    "df_test['remain_ends'] = df_test['remain_ends'].astype(np.float32)\n",
    "df_test['last_stone_is_red'] = df_test['last_stone_is_red'].astype(np.float32)\n",
    "df_test['red_postion'] = df_test['red_postion'].astype(np.float32)\n",
    "display(df_test.dtypes)\n",
    "display(df_test.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b5fa45-0b80-406f-af62-7ea7c0722a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 標準化\n",
    "_stdsc = StandardScaler()\n",
    "##学習時の標準化したパラメータは、評価、本番時におなじ重みで標準化する処理が必要\n",
    "x_test_df = df_test.copy().drop(['filepath','red_diff'],axis=1)\n",
    "sc = pickle.load(open(new_dir_path_recursive+'stdsc_02240209.pkl', \"rb\"))\n",
    "x_test_std = sc.transform(x_test_df)\n",
    "\n",
    "## DataFrameの値を入れ替え\n",
    "qcl = df_test.columns.to_list()\n",
    "qcl.remove('filepath')\n",
    "qcl.remove('red_diff')\n",
    "print(qcl)\n",
    "df_test[qcl] = x_test_std\n",
    "#df_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ac6cb1-332f-4de3-867e-e55994e545bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_test = ImgValueDataset( df_test ,classcol='red_diff' , fncol='filepath' ,transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c903c83-0e72-4f73-9737-b73fd91c2942",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ベストモデル\n",
    "import glob\n",
    "##'model_scripted_{epoch}.smodel.pth'\n",
    "g = glob.glob(new_dir_path_recursive+'model_scripted_*.smodel.pth')\n",
    "g.sort()\n",
    "fn = g[-1]\n",
    "fn = new_dir_path_recursive + 'checkpoint_model.pth'\n",
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bf74aa-ff4d-4650-b22b-9b5415879a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fn)\n",
    "model_from_script = torch.load(fn, map_location=\"cuda\")\n",
    "print(type(model_from_script))\n",
    "net2 = SimpleCNN(11)\n",
    "net2.load_state_dict(torch.load(fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5408b208-6a5f-40cb-a973-0d6d4f0af6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_loader = DataLoader(q_test, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpu_model = net2.to(device)\n",
    "with torch.no_grad():\n",
    "    accs = [] # 各バッチごとの結果格納用\n",
    "    for batch in q_loader:\n",
    "        x, t = batch\n",
    "        x = x.to(device)\n",
    "        t = t.to(device)\n",
    "        y = gpu_model(x)\n",
    "        \n",
    "        y_label = torch.argmax(y, dim=1)\n",
    "        t_label = torch.argmax(t, dim=1)\n",
    "        acc = torch.sum(y_label == t_label) * 1.0 / len(t)\n",
    "        accs.append(acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08465bd-8486-4ca3-ab7f-fdc346c893b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean( list(map( lambda r:r.item() ,accs )) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa9424-73fb-4431-8328-09daa77fa1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14539854-d0eb-44d7-9e66-af2e7b4c3cec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
