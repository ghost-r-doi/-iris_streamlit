{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9423cb9e-02e3-4290-aa66-58774aefd74b",
   "metadata": {},
   "source": [
    "# データローダのテスト \n",
    "画像とスカラー値を同時にあつかえるローダを作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "681a9812-d34e-4918-8012-7af1ac338fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = 'Meiryo'\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b126b55-314b-4af0-a6d5-655343f519bc",
   "metadata": {},
   "source": [
    "## サンプルの作成\n",
    "16x16の画像とファイルパス、スカラ値を２つ以上もつcsvを作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd45c66f-7297-47fb-9d8f-4ea4ead8a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r'./sample/img/'\n",
    "os.makedirs(base_path,exist_ok=True) \n",
    "list = []\n",
    "for i in range(100):\n",
    "    imgdata = np.full((16,16),i*2)\n",
    "    pil_image = Image.fromarray(imgdata.astype(np.uint8))\n",
    "    bfn = os.path.basename(f'img_{i}.png')\n",
    "    out_path = os.path.join( base_path , bfn)\n",
    "    pil_image.save(out_path)\n",
    "    list.append( [i , out_path , i*2 , i**2 ])\n",
    "    pass\n",
    "df = pd.DataFrame(list,columns=['id','path','value1','value2'])\n",
    "csv_path = './sample/dataset.csv'\n",
    "df.to_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50015c67-d269-411d-ab87-fba38d6cd26b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>path</th>\n",
       "      <th>value1</th>\n",
       "      <th>value2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>./sample/img/img_0.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>./sample/img/img_1.png</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                    path  value1  value2\n",
       "0   0  ./sample/img/img_0.png       0       0\n",
       "1   1  ./sample/img/img_1.png       2       1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(csv_path,index_col=0)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "1520303b-5b68-4aa4-946f-cdca2a56ecaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgValPare(Dataset):\n",
    "    def __init__(self, csv_filepath, transform):\n",
    "        ## pandas便利だけど永続化できないので、num_workersが使えなくなる\n",
    "        ## 破棄すること\n",
    "        csv_df = pd.read_csv(csv_path,index_col=0)\n",
    "        csv_df['value1'] = csv_df['value1'].astype(np.float16)\n",
    "        csv_df['value2'] = csv_df['value2'].astype(np.float16)\n",
    "\n",
    "        self.img_pathlist  = csv_df['path'].to_list()\n",
    "        self.val1_list  = csv_df['value1'].to_list()\n",
    "        self.val2_list  = csv_df['value2'].to_list()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):  \n",
    "        return len( self.img_pathlist )\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # 画像をPILとして読み込む\n",
    "        #print(index)\n",
    "        image = Image.open(self.img_pathlist[index])\n",
    "        image = image.convert(\"L\") \n",
    "        #numpy_img = np.asarray(image, np.float32) / 255.0\n",
    "        #tensor_img = np.expand_dims(image, axis=0) # このままだとNWHCの形式\n",
    "        \n",
    "        label = self.val1_list[index]\n",
    "        extend = self.val2_list[index]                         \n",
    "        if self.transform is not None:\n",
    "            ##print('use transform')\n",
    "            image = self.transform(image)\n",
    "        return image, label ,extend\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "e056c2e7-24d8-4a63-a125-a7f6f2eb071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDataset = ImgValPare( csv_path ,transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "f8db9ba5-0e63-469e-8510-fde2e0549974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(myDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "77fb6b51-58a9-456f-8126-4b9f6d62f074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 16])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "9ff425ba-1b5f-4409-b303-ee55bf4fce69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x26cab8ae580>"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ts = transforms.ToPILImage()\n",
    "im = ts(myDataset[0][0])\n",
    "#display(im.size)\n",
    "plt.imshow(np.array(im),  cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "f31d1a79-080b-4e8b-bb32-b572a031f005",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 学習とテストに分割\n",
    "n_samples = len(myDataset)\n",
    "train_size = int( len(myDataset) * 0.8 ) # 教師データのサイズ 全体の80%とする\n",
    "test_size = n_samples - train_size  # テスト用データのサイズ\n",
    "train_data, test_data = torch.utils.data.random_split(\n",
    "        myDataset,\n",
    "        [train_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(0)  # 乱数シードの固定\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "ad5eb6d1-33b8-40cf-be74-3fbbb14e3708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 20)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data) , len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "c437d60e-4084-4f2e-a24a-1897f5b0bb80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "## DataLoader worker (pid(s) 14912) exited unexpectedly メモリ不足\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size,shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "7cb3e83f-78be-43a7-a096-350a792a8cfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 16, 16])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, labels ,extend   = next(iter(train_loader))\n",
    "len(labels)\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26282bc2-4b4e-423b-aeaa-e04b19d747e0",
   "metadata": {},
   "source": [
    "このローダでNetを組んでみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "6847ac5b-7d4d-4f5e-ba86-7ad657a08281",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net( nn.Module ):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1_1 = nn.Conv2d(in_channels=1, out_channels=32 , kernel_size = 3, padding=1 , stride=1)\n",
    "        self.conv1_2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        ## ここで [32, 256, 4, 4]\n",
    "        self.fc1 = nn.Linear(in_features= (256*4*4) + 1, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=1)\n",
    "    def forward(self, x_and_extend):\n",
    "        x =x_and_extend[0]\n",
    "        x2 = x_and_extend[1]\n",
    "        #print('in',x.shape)\n",
    "        x = F.relu(self.conv1_1(x))\n",
    "        #print('conv1_1',x.shape)\n",
    "        x = F.relu(self.conv1_2(x))\n",
    "        #print('conv1_2',x.shape)\n",
    "        x = self.pool1(x)\n",
    "        #print('pool1',x.shape , len(x.flatten()) )\n",
    "        \n",
    "        x = F.relu(self.conv2_1(x))\n",
    "        x = F.relu(self.conv2_2(x))\n",
    "        x = self.pool2(x)\n",
    "        #print('pool2',x.shape ,  len(x.flatten()))\n",
    "        \n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        print('view',x.shape , len(x.flatten()))\n",
    "        print(x)\n",
    "        print('x2',x2.shape )\n",
    "        x3 = x2.unsqueeze(dim=1)\n",
    "        print('x3',x3.shape )\n",
    "        x = torch.cat([x, x3], dim=1) \n",
    "        print(x.dtype , x3.dtype)\n",
    "        print('cat',x.shape , len(x.flatten()))\n",
    "        print(x)\n",
    "        x = self.fc1(x)\n",
    "        print('fc1',x.shape ,  len(x.flatten()))\n",
    "        x = self.fc2(x)\n",
    "        print('fc2',x.shape ,  len(x.flatten()))\n",
    "        \n",
    "        return x  \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        #print(num_features)\n",
    "        return num_features\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "44b2fec6-e2ac-4ec8-bdab-a672850a3290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1_1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv1_2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2_2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=4097, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "ef8a5b0a-d691-4c7e-ae84-e1b6b4a7d6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 損失関数\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "b9b9b50a-43f0-4e30-b2b9-b04cbedf41bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 最適化\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "b403817f-d295-4f00-ad87-1f506eb541b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Epoch 1/30\n",
      "-------------\n",
      "view torch.Size([32, 4096]) 131072\n",
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0285, 0.0287, 0.0195],\n",
      "        [0.0035, 0.0000, 0.0000,  ..., 0.0611, 0.0578, 0.0603],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0332, 0.0324, 0.0252],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0319, 0.0315, 0.0234],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0345, 0.0339, 0.0277],\n",
      "        [0.0003, 0.0000, 0.0000,  ..., 0.0552, 0.0520, 0.0531]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "x2 torch.Size([32])\n",
      "x3 torch.Size([32, 1])\n",
      "torch.float32 torch.float32\n",
      "cat torch.Size([32, 4097]) 131104\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.8702e-02, 1.9480e-02,\n",
      "         6.4000e+01],\n",
      "        [3.4673e-03, 0.0000e+00, 0.0000e+00,  ..., 5.7821e-02, 6.0253e-02,\n",
      "         4.9000e+03],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.2382e-02, 2.5158e-02,\n",
      "         3.6100e+02],\n",
      "        ...,\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.1469e-02, 2.3442e-02,\n",
      "         2.2500e+02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.3899e-02, 2.7733e-02,\n",
      "         6.2500e+02],\n",
      "        [2.6826e-04, 0.0000e+00, 0.0000e+00,  ..., 5.2046e-02, 5.3128e-02,\n",
      "         3.7200e+03]], grad_fn=<CatBackward0>)\n",
      "fc1 torch.Size([32, 128]) 4096\n",
      "fc2 torch.Size([32, 1]) 32\n",
      "view torch.Size([32, 4096]) 131072\n",
      "tensor([[0.0341, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0267, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0130, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0404, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0145, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "x2 torch.Size([32])\n",
      "x3 torch.Size([32, 1])\n",
      "torch.float32 torch.float32\n",
      "cat torch.Size([32, 4097]) 131104\n",
      "tensor([[3.4081e-02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         6.5600e+03],\n",
      "        [2.6715e-02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         4.7600e+03],\n",
      "        [1.3047e-02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         1.9360e+03],\n",
      "        ...,\n",
      "        [4.0400e-02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         8.2800e+03],\n",
      "        [1.4517e-02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         2.2080e+03],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+02]], grad_fn=<CatBackward0>)\n",
      "fc1 torch.Size([32, 128]) 4096\n",
      "fc2 torch.Size([32, 1]) 32\n",
      "view torch.Size([16, 4096]) 65536\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<ViewBackward0>)\n",
      "x2 torch.Size([16])\n",
      "x3 torch.Size([16, 1])\n",
      "torch.float32 torch.float32\n",
      "cat torch.Size([16, 4097]) 65552\n",
      "tensor([[   0.,    0.,    0.,  ...,    0.,    0., 2808.],\n",
      "        [   0.,    0.,    0.,  ...,    0.,    0., 9408.],\n",
      "        [   0.,    0.,    0.,  ...,    0.,    0.,  144.],\n",
      "        ...,\n",
      "        [   0.,    0.,    0.,  ...,    0.,    0., 8100.],\n",
      "        [   0.,    0.,    0.,  ...,    0.,    0., 2116.],\n",
      "        [   0.,    0.,    0.,  ...,    0.,    0.,   81.]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "fc1 torch.Size([16, 128]) 2048\n",
      "fc2 torch.Size([16, 1]) 16\n",
      "Loss: nan Acc: 0.0125\n"
     ]
    }
   ],
   "source": [
    "##https://venoda.hatenablog.com/entry/2020/10/14/071440\n",
    "##https://aidiary.hatenablog.com/entry/20180221/1519223357\n",
    "\n",
    "num_epochs = 30\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "net = net.to(device)\n",
    "net.train()\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "    print('-------------')\n",
    "    net.train()\n",
    "    # 損失和\n",
    "    epoch_loss = 0.0\n",
    "    # epochの正解数\n",
    "    epoch_corrects = 0\n",
    "    for inputs, labels ,extend in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        extend = extend.to(device,dtype=torch.float32)\n",
    "        # 勾配を初期化する\n",
    "        optimizer.zero_grad()\n",
    "        # 学習時のみ勾配を計算させる設定にする\n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs = net([inputs,extend])\n",
    "             # ラベルを予測\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            # 損失関数を使って損失を計算する\n",
    "            loss = criterion(outputs.to(torch.float64), labels)\n",
    "             # 誤差を逆伝搬する\n",
    "            loss.backward()\n",
    "            # パラメータを更新する\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * inputs.size(0)\n",
    "            epoch_corrects += torch.sum(preds == labels.data)\n",
    "    # 1エポックでの損失を計算\n",
    "    epoch_loss = epoch_loss / len(train_loader.dataset)\n",
    "    # 1エポックでの正解率を計算\n",
    "    epoch_acc = epoch_corrects.double() / len(train_loader.dataset)\n",
    "    print('Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "    break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "c23a5071-ec89-454c-985e-edaa69676f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extend.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "4de855d6-a443-488f-8716-9752667ccc81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "e4e7616c-1b06-4041-b317-0af491627cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "5bd26292-cb58-4682-82fb-ccabbbf6ffa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.6100e+02, 6.8880e+03, 9.6000e+03, 4.3560e+03, 5.2900e+02, 6.2500e+02,\n",
       "        6.0840e+03, 3.6000e+01, 7.2240e+03, 8.8320e+03, 3.8440e+03, 2.5600e+02,\n",
       "        5.7760e+03, 1.9360e+03, 0.0000e+00, 8.1000e+03, 9.0240e+03, 5.7600e+02,\n",
       "        1.0000e+00, 7.9200e+03, 1.3690e+03, 7.8400e+02, 2.5000e+01, 1.2100e+02,\n",
       "        3.0240e+03, 2.7040e+03, 5.9280e+03, 1.2960e+03, 8.4100e+02, 6.5600e+03,\n",
       "        1.7640e+03, 9.2160e+03], dtype=torch.float64)"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a21301-4aae-4b4f-9832-c35988c1fe6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
