{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c4b3af0-8796-4f91-a3b8-a189ed4b5b47",
   "metadata": {},
   "source": [
    "# 自前のネットワークを組んで学習させてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7703dc5b-ee9d-4aaa-add2-f96d5029ac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "plt.rcParams['font.family'] = 'Meiryo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41647b1b-afa7-4fd6-ae50-20a1694f1918",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn = r'./dataset_cur_train.csv'\n",
    "test_fn = r'./dataset_cur_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5294d05-3e0d-461d-83fc-16ebda6d1449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "red_diff             float32\n",
       "remain_ends          float32\n",
       "last_stone_is_red    float32\n",
       "red_postion          float32\n",
       "filepath              object\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>red_diff</th>\n",
       "      <th>remain_ends</th>\n",
       "      <th>last_stone_is_red</th>\n",
       "      <th>red_postion</th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>./dataset_o\\ECC2023_ResultsBook_Women_A-Divisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>./dataset_o\\ECC2021_ResultsBook_Men_A-Division...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>./dataset_o\\ECC2019_ResultsBook_Women_A-Divisi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   red_diff  remain_ends  last_stone_is_red  red_postion  \\\n",
       "8       1.0          0.0                1.0          3.0   \n",
       "0       2.0          8.0                1.0          0.0   \n",
       "8      -2.0          0.0                0.0          3.0   \n",
       "\n",
       "                                            filepath  \n",
       "8  ./dataset_o\\ECC2023_ResultsBook_Women_A-Divisi...  \n",
       "0  ./dataset_o\\ECC2021_ResultsBook_Men_A-Division...  \n",
       "8  ./dataset_o\\ECC2019_ResultsBook_Women_A-Divisi...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = pd.read_csv(train_fn,index_col=0)\n",
    "df_train['red_diff'] = df_train['red_diff'].astype(np.float32)\n",
    "df_train['remain_ends'] = df_train['remain_ends'].astype(np.float32)\n",
    "df_train['last_stone_is_red'] = df_train['last_stone_is_red'].astype(np.float32)\n",
    "df_train['red_postion'] = df_train['red_postion'].astype(np.float32)\n",
    "display(df_train.dtypes)\n",
    "display(df_train.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2416d239-816d-4246-98c9-68aa35baafb0",
   "metadata": {},
   "source": [
    "## red_diff が目的、fn,remain_ends,last_stone_is_red,red_postion が説明変数\n",
    "説明変数を標準化する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba7ce319-2e82-424f-94ff-2a6171635c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5558919 ,  0.9972611 ,  1.2323233 ],\n",
       "       [ 1.5270197 ,  0.9972611 , -0.01566701],\n",
       "       [-1.5558919 , -1.0027465 ,  1.2323233 ]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['remain_ends', 'last_stone_is_red', 'red_postion']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([4.0374607 , 0.50137133, 0.03766138])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([6.73376293, 0.24999812, 5.7785669 ])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>red_diff</th>\n",
       "      <th>remain_ends</th>\n",
       "      <th>last_stone_is_red</th>\n",
       "      <th>red_postion</th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.555892</td>\n",
       "      <td>0.997261</td>\n",
       "      <td>1.232323</td>\n",
       "      <td>./dataset_o\\ECC2023_ResultsBook_Women_A-Divisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.527020</td>\n",
       "      <td>0.997261</td>\n",
       "      <td>-0.015667</td>\n",
       "      <td>./dataset_o\\ECC2021_ResultsBook_Men_A-Division...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.555892</td>\n",
       "      <td>-1.002746</td>\n",
       "      <td>1.232323</td>\n",
       "      <td>./dataset_o\\ECC2019_ResultsBook_Women_A-Divisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.170528</td>\n",
       "      <td>0.997261</td>\n",
       "      <td>-0.431664</td>\n",
       "      <td>./dataset_o\\OWG2018_ResultsBook\\geme835end8.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-3.0</td>\n",
       "      <td>-1.555892</td>\n",
       "      <td>-1.002746</td>\n",
       "      <td>0.400330</td>\n",
       "      <td>./dataset_o\\WWCC2018_ResultsBook\\geme338end9.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   red_diff  remain_ends  last_stone_is_red  red_postion  \\\n",
       "8       1.0    -1.555892           0.997261     1.232323   \n",
       "0       2.0     1.527020           0.997261    -0.015667   \n",
       "8      -2.0    -1.555892          -1.002746     1.232323   \n",
       "7       1.0    -1.170528           0.997261    -0.431664   \n",
       "8      -3.0    -1.555892          -1.002746     0.400330   \n",
       "\n",
       "                                            filepath  \n",
       "8  ./dataset_o\\ECC2023_ResultsBook_Women_A-Divisi...  \n",
       "0  ./dataset_o\\ECC2021_ResultsBook_Men_A-Division...  \n",
       "8  ./dataset_o\\ECC2019_ResultsBook_Women_A-Divisi...  \n",
       "7    ./dataset_o\\OWG2018_ResultsBook\\geme835end8.png  \n",
       "8   ./dataset_o\\WWCC2018_ResultsBook\\geme338end9.png  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 標準化\n",
    "stdsc = StandardScaler()\n",
    "##学習時の標準化したパラメータは、評価、本番時におなじ重みで標準化する処理が必要\n",
    "x_train_df = df_train.copy().drop(['filepath','red_diff'],axis=1)\n",
    "x_train_std = stdsc.fit_transform(x_train_df)\n",
    "display( x_train_std[:3] )\n",
    "## DataFrameの値を入れ替え\n",
    "qcl = df_train.columns.to_list()\n",
    "qcl.remove('filepath')\n",
    "qcl.remove('red_diff')\n",
    "print(qcl)\n",
    "df_train[qcl] = x_train_std\n",
    "pickle.dump(stdsc, open(\"stdsc_02240212.pkl\", \"wb\"))\n",
    "display(stdsc.n_features_in_, stdsc.mean_ , stdsc.var_) \n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232f4a74-d1e7-4304-a0f7-933485b4a9ba",
   "metadata": {},
   "source": [
    "### 20240209_samplescreeningで画像の平均 分散は計算ずみ\n",
    "平均:[0.5254706740379333, 0.5207158327102661, 0.5373605489730835]\r",
    " \r",
    "分散[0.47727182507514954, 0.4752059578895569, 0.4878864884376526] :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc251581-3052-406f-94bf-c781ea7943a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d98501e-1d3d-4b23-bb83-646cf5a3f981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[300, 540]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "120.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 画像変換の定義\n",
    "w,h = Image.open(df_train['filepath'].values[0]).size\n",
    "## 正方形にするための差分\n",
    "pad = (h-w)/2\n",
    "display([w,h],pad)\n",
    "target_size = 224\n",
    "## train用\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Pad(( 240 // 2, 0), fill=0, padding_mode='constant'),  # 左右に余白を追加\n",
    "    transforms.Resize(target_size),\n",
    "    transforms.CenterCrop(target_size),\n",
    "    transforms.RandomHorizontalFlip(0.33),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5254, 0.521, 0.538], std=[0.477, 0.475, 0.487])\n",
    "])\n",
    "# valid/test用\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Pad(( 240 // 2, 0), fill=0, padding_mode='constant'),  # 左右に余白を追加\n",
    "    transforms.Resize(target_size),\n",
    "    transforms.CenterCrop(target_size),\n",
    "    transforms.RandomHorizontalFlip(0.33),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5254, 0.521, 0.538], std=[0.477, 0.475, 0.487])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22f5154d-84a6-42eb-b638-dbab73d94516",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9f773c-6504-40e9-a103-bd8b100ab1a2",
   "metadata": {},
   "source": [
    "# 自前のデータセット定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f04521e7-c7bd-4a5d-9a0c-71a00502dff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgValueDataset(Dataset):\n",
    "    def __init__(self, df, classcol , fncol , transform):\n",
    "        \n",
    "        ##self.label_list  = df[classcol].to_list()\n",
    "        class_data = pd.get_dummies(df[classcol]).values\n",
    "        self.label_list  = class_data.astype(float)\n",
    "\n",
    "        self.val_list  = df[['remain_ends','last_stone_is_red','red_postion']].astype(np.float16).values\n",
    "        \n",
    "        self.img_pathlist  = df[fncol].to_list()\n",
    "        cols = df.columns.to_list()\n",
    "        cols.remove(fncol)\n",
    "        cols.remove(classcol)\n",
    "        self.x_values = df[cols].values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):  \n",
    "        return len( self.img_pathlist )\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # 画像をPILとして読み込む\n",
    "        #print(index,self.img_pathlist[index])\n",
    "        image = Image.open(self.img_pathlist[index])\n",
    "        \n",
    "        label = self.label_list[index]\n",
    "\n",
    "        extend = self.val_list[index]                         \n",
    "        if self.transform is not None:\n",
    "            ##print('use transform')\n",
    "            image = self.transform(image)\n",
    "        ## 次元を足してやってっそこに追加データをぶっこむ\n",
    "        extend_tensor = np.full((224,224),255)\n",
    "        extend_tensor[1][0] = extend[0]\n",
    "        extend_tensor[1][1] = extend[1]\n",
    "        extend_tensor[1][2] = extend[2]\n",
    "        #print(extend_tensor)\n",
    "        extend_tensor = torch.Tensor(extend_tensor)\n",
    "        extend_tensor = extend_tensor.unsqueeze(0)\n",
    "        out = torch.cat([image, extend_tensor], dim=0)\n",
    "                             \n",
    "        return out, label \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1a795a1-a4b0-4f93-bb8d-9544cce15f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11959"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df_train, test_size=0.2, stratify=df_train['red_diff'])\n",
    "train_dataset = ImgValueDataset( train_df ,classcol='red_diff' , fncol='filepath',transform=transform_train)\n",
    "test_dataset = ImgValueDataset( test_df ,classcol='red_diff' , fncol='filepath' ,transform=transform_test)\n",
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a9aa0e0-f19c-4983-846a-4fb6955f5645",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f12baa4-eaf9-43e9-aed0-30be6dabc706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2990"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b80edabd-3b50-49e8-aec9-88cb63cfa1b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 4, 224, 224]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, labels = next(iter(train_loader))\n",
    "inputs.shape , labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29ca2083-5c34-403f-9d51-ec37ea1750f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 224, 224])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = inputs[:3]\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10aa115c-2356-43c7-a7b5-58c3fe577e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[255., 255., 255.,  ..., 255., 255., 255.],\n",
       "        [  0.,  -1.,   0.,  ..., 255., 255., 255.],\n",
       "        [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "        ...,\n",
       "        [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "        [255., 255., 255.,  ..., 255., 255., 255.],\n",
       "        [255., 255., 255.,  ..., 255., 255., 255.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[2,3,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "239d9d73-c67f-469a-aca2-649351d9deb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 224, 224]) tensor([[1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "values = inputs[3:]\n",
    "print(values.shape , values[:,3,1,0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0d21d0a-ca15-4f11-81b1-6603377007d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1., -2.,  2.,  0.,  3., -4.,  1.,  5., -3.,  4., -5.],\n",
       "       dtype=float32),\n",
       " 11)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_list = train_df['red_diff'].unique()\n",
    "class_list , len(class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d03684ed-0a69-4157-a7d4-ceb1bd546cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0., -1.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0., -1.,  0.],\n",
       "        [ 1.,  0.,  0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[:,3,1,0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14f0cda-b0ce-4e75-a54d-7cbc9ac14da9",
   "metadata": {},
   "source": [
    "# ネットを組む  \n",
    "https://qiita.com/poorko/items/c151ff4a827f114fe954"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1b9b5f2-4064-4e3d-861a-bc23310ac89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27da4588-b7ef-42ff-a8c2-0951db5e44d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNNモデルの定義\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 12, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(12, 24, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(24, 48, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        viewCnt = (48 * 28 * 28)\n",
    "        \n",
    "        self.fc1 = nn.Linear(viewCnt+3, 256)\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, out_channels=12, kernel_size=5, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(12)\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=5, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(12)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=5, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(24)\n",
    "        self.conv5 = nn.Conv2d(in_channels=24, out_channels=24, kernel_size=5, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(24)\n",
    "\n",
    "        viewCnt = 24 * 106 * 106\n",
    "        \n",
    "        self.fc1 = nn.Linear(viewCnt+3, 10240)\n",
    "        self.Dropout1 = nn.Dropout(p=0.2, inplace=False)\n",
    "        self.fc2 = nn.Linear(10240, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x\n",
    "        #print('input:',input.shape)\n",
    "        images = input[:,:3]\n",
    "\n",
    "        output = F.relu(self.bn1(self.conv1(images)))      \n",
    "        output = F.relu(self.bn2(self.conv2(output)))     \n",
    "        output = self.pool(output)                        \n",
    "        output = F.relu(self.bn4(self.conv4(output)))     \n",
    "        output = F.relu(self.bn5(self.conv5(output)))   \n",
    "\n",
    "        output = output.view(-1, viewCnt)\n",
    "\n",
    "        ###print(output.shape)\n",
    "        params = input[:,3,1,0:3]\n",
    "        #print(output.shape,output.dtype)\n",
    "        #print(params.shape,params.dtype)\n",
    "        packedData = torch.cat([output,params],1)\n",
    "        #print(output.shape,params.shape,packedData.shape)\n",
    "        output = F.relu(self.fc1(packedData))\n",
    "        output = self.dropout(output)\n",
    "        output = F.relu(self.fc2(output))\n",
    "        output = F.relu(self.fc3(output))\n",
    "      \n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44185bb6-5b7a-41fc-bb5b-37e21e21a5b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (conv1): Conv2d(3, 12, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(12, 12, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(24, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=269667, out_features=10240, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=10240, out_features=128, bias=True)\n",
      "  (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv2d(12, 24, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv5): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (bn5): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (Dropout1): Dropout(p=0.2, inplace=False)\n",
      "  (fc3): Linear(in_features=128, out_features=11, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = SimpleCNN(11)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec0cdb6-eb60-4e7f-affa-9543292a91f7",
   "metadata": {},
   "source": [
    "EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "176fad73-44a8-4be0-adec-1a7b5b9f13e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"earlystoppingクラス\"\"\"\n",
    "\n",
    "    def __init__(self, patience=5, verbose=False, path='checkpoint_model.pth'):\n",
    "        \"\"\"引数：最小値の非更新数カウンタ、表示設定、モデル格納path\"\"\"\n",
    "\n",
    "        self.patience = patience    #設定ストップカウンタ\n",
    "        self.verbose = verbose      #表示の有無\n",
    "        self.counter = 0            #現在のカウンタ値\n",
    "        self.best_score = None      #ベストスコア\n",
    "        self.early_stop = False     #ストップフラグ\n",
    "        self.val_loss_min = np.Inf   #前回のベストスコア記憶用\n",
    "        self.path = path             #ベストモデル格納path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        特殊(call)メソッド\n",
    "        実際に学習ループ内で最小lossを更新したか否かを計算させる部分\n",
    "        \"\"\"\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:  #1Epoch目の処理\n",
    "            self.best_score = score   #1Epoch目はそのままベストスコアとして記録する\n",
    "            self.checkpoint(val_loss, model)  #記録後にモデルを保存してスコア表示する\n",
    "        elif score < self.best_score:  # ベストスコアを更新できなかった場合\n",
    "            self.counter += 1   #ストップカウンタを+1\n",
    "            if self.verbose:  #表示を有効にした場合は経過を表示\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')  #現在のカウンタを表示する \n",
    "            if self.counter >= self.patience:  #設定カウントを上回ったらストップフラグをTrueに変更\n",
    "                self.early_stop = True\n",
    "        else:  #ベストスコアを更新した場合\n",
    "            self.best_score = score  #ベストスコアを上書き\n",
    "            self.checkpoint(val_loss, model)  #モデルを保存してスコア表示\n",
    "            self.counter = 0  #ストップカウンタリセット\n",
    "\n",
    "    def checkpoint(self, val_loss, model):\n",
    "        '''ベストスコア更新時に実行されるチェックポイント関数'''\n",
    "        if self.verbose:  #表示を有効にした場合は、前回のベストスコアからどれだけ更新したか？を表示\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)  #ベストモデルを指定したpathに保存\n",
    "        self.val_loss_min = val_loss  #その時のlossを記録する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a382c74a-027d-41b9-a696-ae720728ab62",
   "metadata": {},
   "source": [
    "## 訓練（ネットワークが通るか確認）  \n",
    "https://atmarkit.itmedia.co.jp/ait/articles/2006/12/news021.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7972682-c9f2-40d5-ace5-b65335e99cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(values1, values2, rng, label1, label2):\n",
    "    plt.plot(range(rng), values1, label=label1)\n",
    "    plt.plot(range(rng), values2, label=label2)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "992b7ddd-0733-45d8-a7de-8dfaae7afde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, dataloader, criterion, optimizer,device):\n",
    "    net.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        #print('入力:',inputs.shape , labels.shape,labels.dtype)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        #print('評価:',outputs.shape , labels.shape,labels.dtype)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        _, ons_labels = torch.max(labels, 1)\n",
    "        total_correct += (predicted == ons_labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    accuracy = total_correct / len(dataloader.dataset)\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def validate(net, dataloader, criterion,device):\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, ons_labels = torch.max(labels, 1)\n",
    "            total_correct += (predicted == ons_labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    avg_accuracy = total_correct / len(dataloader.dataset)\n",
    "\n",
    "    return avg_loss, avg_accuracy ,total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1bda984b-5b9b-4047-947d-0fdeab431862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_train_and_validate(net, trainset, criterion, optimizer, epochs,device):\n",
    "    best_accuracy = 0.0\n",
    "    trainloader, validloader = trainset\n",
    "    curEarlyStopping = EarlyStopping(patience=5, verbose=True)\n",
    "    \n",
    "    history = {}\n",
    "    history['train_loss_values'] = []\n",
    "    history['train_accuracy_values'] = []\n",
    "    history['valid_loss_values'] = []\n",
    "    history['valid_accuracy_values'] = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        current_time = datetime.now()\n",
    "        formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f'epoch: {epoch:3} <{formatted_time}>')\n",
    "\n",
    "        t_loss, t_accu = train(net, trainloader, criterion, optimizer,device)\n",
    "        v_loss, v_accu, total_loss = validate(net, validloader, criterion,device)\n",
    "\n",
    "        print(f'train_loss: {t_loss:.6f}, train_accuracy: {t_accu:3.4%},',\n",
    "              f'valid_loss: {v_loss:.6f}, valid_accuracy: {v_accu:3.4%}')\n",
    "\n",
    "        history['train_loss_values'].append(t_loss)\n",
    "        history['train_accuracy_values'].append(t_accu)\n",
    "        history['valid_loss_values'].append(v_loss)\n",
    "        history['valid_accuracy_values'].append(v_accu)\n",
    "        if best_accuracy < v_accu:\n",
    "            best_accuracy = v_accu\n",
    "            model_scripted = torch.jit.script(net)\n",
    "            model_scripted.save(f'xl_model_scripted_{epoch}.smodel.pth')\n",
    "\n",
    "         #★毎エポックearlystoppingの判定をさせる★\n",
    "        curEarlyStopping( v_loss , net) #callメソッド呼び出し\n",
    "        if earlystopping.early_stop: #ストップフラグがTrueの場合、breakでforループを抜ける\n",
    "            print(\"Early Stopping!\")\n",
    "            return history\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1ffae3a-8405-4241-af03-baac72b2b633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'optim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m net \u001b[38;5;241m=\u001b[39m SimpleCNN(\u001b[38;5;241m11\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m---> 13\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43moptim\u001b[49m\u001b[38;5;241m.\u001b[39mSGD(net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[0;32m     14\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m\n\u001b[0;32m     15\u001b[0m trainset \u001b[38;5;241m=\u001b[39m [train_loader,test_loader]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'optim' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset = ImgValueDataset( train_df ,classcol='red_diff' , fncol='filepath',transform=transform_train)\n",
    "test_dataset = ImgValueDataset( test_df ,classcol='red_diff' , fncol='filepath' ,transform=transform_test)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "net = SimpleCNN(11).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "EPOCHS = 300\n",
    "trainset = [train_loader,test_loader]\n",
    "\n",
    "history = do_train_and_validate(net, trainset, criterion, optimizer, EPOCHS,device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70005ff-3f5c-4062-a0ca-f193722dd117",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_losses = history['train_loss_values']\n",
    "t_accus = history['train_accuracy_values']\n",
    "v_losses = history['valid_loss_values']\n",
    "v_accus = history['valid_accuracy_values']\n",
    "\n",
    "plot_graph(t_losses, v_losses, EPOCHS, 'loss(train)', 'loss(validate)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0472e50-93a4-48f5-be84-3c8c38ed54c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(t_accus, v_accus, EPOCHS, 'accuracy(train)', 'accuracy(validate)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ef34ff-271c-40b8-86c8-584babfeca97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b7ff060-9a66-475e-b945-232d7afd9700",
   "metadata": {},
   "source": [
    "## 取り置きのテスト画像群"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3024cdab-7bc8-4c08-a737-90d25fcdfc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fn = r'./dataset_cur_test.csv'\n",
    "test_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b04518-7ea4-418d-8034-ce04bd42988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(test_fn,index_col=0)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "df_test['red_diff'] = df_test['red_diff'].astype(np.float32)\n",
    "df_test['remain_ends'] = df_test['remain_ends'].astype(np.float32)\n",
    "df_test['last_stone_is_red'] = df_test['last_stone_is_red'].astype(np.float32)\n",
    "df_test['red_postion'] = df_test['red_postion'].astype(np.float32)\n",
    "display(df_test.dtypes)\n",
    "display(df_test.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b5fa45-0b80-406f-af62-7ea7c0722a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 標準化\n",
    "_stdsc = StandardScaler()\n",
    "##学習時の標準化したパラメータは、評価、本番時におなじ重みで標準化する処理が必要\n",
    "x_test_df = df_test.copy().drop(['filepath','red_diff'],axis=1)\n",
    "sc = pickle.load(open('stdsc_02240212.pkl', \"rb\"))\n",
    "x_test_std = sc.transform(x_test_df)\n",
    "\n",
    "## DataFrameの値を入れ替え\n",
    "qcl = df_test.columns.to_list()\n",
    "qcl.remove('filepath')\n",
    "qcl.remove('red_diff')\n",
    "print(qcl)\n",
    "df_test[qcl] = x_test_std\n",
    "#df_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ac6cb1-332f-4de3-867e-e55994e545bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_test = ImgValueDataset( df_test ,classcol='red_diff' , fncol='filepath' ,transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b40d5b9-e36b-4c9f-b5ff-1f5da6dd3500",
   "metadata": {},
   "outputs": [],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c903c83-0e72-4f73-9737-b73fd91c2942",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ベストモデル\n",
    "import glob\n",
    "##'model_scripted_{epoch}.smodel.pth'\n",
    "g = glob.glob('xl_model_scripted_*.smodel.pth')\n",
    "g.sort()\n",
    "fn = g[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bf74aa-ff4d-4650-b22b-9b5415879a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fn)\n",
    "model_from_script = torch.jit.load(fn, map_location=\"cuda\")\n",
    "print(type(model_from_script))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5408b208-6a5f-40cb-a973-0d6d4f0af6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_loader = DataLoader(q_test, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpu_model = model_from_script.to(device)\n",
    "with torch.no_grad():\n",
    "    accs = [] # 各バッチごとの結果格納用\n",
    "    for batch in q_loader:\n",
    "        x, t = batch\n",
    "        x = x.to(device)\n",
    "        t = t.to(device)\n",
    "        y = gpu_model(x)\n",
    "        \n",
    "        y_label = torch.argmax(y, dim=1)\n",
    "        t_label = torch.argmax(t, dim=1)\n",
    "        acc = torch.sum(y_label == t_label) * 1.0 / len(t)\n",
    "        accs.append(acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08465bd-8486-4ca3-ab7f-fdc346c893b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean( list(map( lambda r:r.item() ,accs )) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa9424-73fb-4431-8328-09daa77fa1d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
